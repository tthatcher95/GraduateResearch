{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from operator import itemgetter\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def native_vote_csv_df():\n",
    "    # filepath = \"/Users/tdt62/Desktop/test_data/2018_10_*vote*\"\n",
    "    list_ = []\n",
    "\n",
    "#     filepath = \"/projects/canis/nativevote18/twitter/data/2018_10*stream_1*\"\n",
    "    filepath = \"/Users/tdt62/Desktop/GraduateResearch/test_data/2018_10_*vote*\"\n",
    "\n",
    "    # Takes all of the csv file and makes one big dataframe\n",
    "    for name in glob.glob(filepath):\n",
    "        if(os.stat(name).st_size == 0) == True:\n",
    "            continue\n",
    "        else:\n",
    "            df = pd.read_csv(name,index_col=None, header=0)\n",
    "            list_.append(df)\n",
    "\n",
    "#     filepath = \"/projects/canis/nativevote18/twitter/data/2018_11*stream_1*\"\n",
    "# #     filepath = \"/Users/tdt62/Desktop/GraduateResearch/test_data/2018_11_*vote*\"\n",
    "\n",
    "#     # Takes all of the csv file and makes one big dataframe\n",
    "#     for name in glob.glob(filepath):\n",
    "#         if(os.stat(name).st_size == 0) == True:\n",
    "#             continue\n",
    "#         else:\n",
    "#             df = pd.read_csv(name,index_col=None, header=0)\n",
    "#             list_.append(df)\n",
    "\n",
    "#     filepath = \"/projects/canis/nativevote18/twitter/data/2018_12*stream_1*\"\n",
    "# #     filepath = \"/Users/tdt62/Desktop/test_data/2018_12_01*vote*\"\n",
    "\n",
    "#         # Takes all of the csv file and makes one big dataframe\n",
    "#         for name in glob.glob(filepath):\n",
    "#             if(os.stat(name).st_size == 0) == True:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 df = pd.read_csv(name,index_col=None, header=0)\n",
    "#                 list_.append(df)\n",
    "\n",
    "#         filepath = \"/projects/canis/nativevote18/twitter/data/2019_01*stream_1*\"\n",
    "\n",
    "#     # Takes all of the csv file and makes one big dataframe\n",
    "#     for name in glob.glob(filepath):\n",
    "#         if(os.stat(name).st_size == 0) == True:\n",
    "#             continue\n",
    "#         else:\n",
    "#             df = pd.read_csv(name,index_col=None, header=0)\n",
    "#             list_.append(df)\n",
    "\n",
    "    # Makes the big df in memory\n",
    "    frame = pd.concat(list_, axis = 0, ignore_index = True)\n",
    "    frame.fillna(\"NA\", inplace=True)\n",
    "    return frame\n",
    "\n",
    "def total_tweets(df):\n",
    "\n",
    "    # Gets the total number of tweets\n",
    "    total_tweets_num = df.shape[0]  # gives number of row count\n",
    "    return total_tweets_num\n",
    "\n",
    "def unique_tweets(df):\n",
    "\n",
    "    # Gets the unique tweets i.e. no retweeted status\n",
    "    df['rt_isdigit'] = list(map(lambda x: str(x).isdigit(), df['Retweeted_Status']))\n",
    "    unique_df = df.loc[df['rt_isdigit'] == False]\n",
    "    return unique_df\n",
    "\n",
    "def unique_users(df):\n",
    "    unique_users = len(df.User_ID.unique())\n",
    "    return unique_users\n",
    "\n",
    "def original_content_user(df):\n",
    "\n",
    "    # Gets the unique tweets i.e. no retweeted status\n",
    "    df['rt_isdigit'] = list(map(lambda x: str(x).isdigit(), df['Retweeted_Status']))\n",
    "    unique_df = df.loc[df['rt_isdigit'] == False]\n",
    "    unique_users = len(unique_df.User_ID.unique())\n",
    "    return unique_users\n",
    "\n",
    "def hashtags(df, top_val):\n",
    "\n",
    "    top_list = []\n",
    "    hashtag_dict = {}\n",
    "\n",
    "    def iterate_hashtags(x):\n",
    "        hashtag_list = list(x.split(\"'text':\"))\n",
    "        for element in hashtag_list:\n",
    "            stripped_element = element.split(',')[0].strip(\"' '{}[]\")\n",
    "            if stripped_element in hashtag_dict and stripped_element != '[]' and stripped_element != 'NA' and stripped_element != '':\n",
    "                hashtag_dict[stripped_element] += 1\n",
    "            else:\n",
    "                hashtag_dict[stripped_element] = 1\n",
    "\n",
    "    # Create the hashtag dict\n",
    "    list(map(lambda x: iterate_hashtags(x), df['Hashtags']))\n",
    "\n",
    "    # Gets the sorted news_list in descending order\n",
    "    sorted_news_list = (list(sorted(hashtag_dict.items(), key=itemgetter(1), reverse=True)))\n",
    "\n",
    "    # Gets the top 20 results of the sorted list\n",
    "    sorted_news_list = sorted_news_list[0:top_val]\n",
    "\n",
    "    # Stores all the names in a list, so we can read correctky\n",
    "    for item in sorted_news_list:\n",
    "        top_list.append(item[0])\n",
    "\n",
    "    return top_list, sorted_news_list\n",
    "\n",
    "def make_domain_csv(top_news_dict):\n",
    "    with open('news_domains.csv', 'w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        for key, value in top_news_dict.items():\n",
    "            writer.writerow([key, value])\n",
    "\n",
    "def top_user_mentions(df, top_val):\n",
    "\n",
    "    top_users_list = []\n",
    "    hashtag_dict = {}\n",
    "\n",
    "    def iterate_user_mentions(x):\n",
    "\n",
    "        hashtag_list = list(x.split(\"'screen_name':\"))\n",
    "        for element in hashtag_list:\n",
    "            stripped_element = element.split(',')[0].strip(\"' '{}[]\")\n",
    "            if stripped_element in hashtag_dict and stripped_element != '[]' and stripped_element != 'NA' and stripped_element != '':\n",
    "                hashtag_dict[stripped_element] += 1\n",
    "            else:\n",
    "                hashtag_dict[stripped_element] = 1\n",
    "\n",
    "    # Create the hashtag dict\n",
    "    list(map(lambda x: iterate_user_mentions(x), df['User_Mentions']))\n",
    "\n",
    "    # Gets the sorted news_list in descending order\n",
    "    sorted_news_list = (list(sorted(hashtag_dict.items(), key=itemgetter(1), reverse=True)))\n",
    "\n",
    "    # Gets the top 20 results of the sorted list\n",
    "    sorted_news_list = sorted_news_list[0:top_val]\n",
    "\n",
    "    # Stores all the names in a list, so we can read correctky\n",
    "    for item in sorted_news_list:\n",
    "        top_users_list.append(item[0])\n",
    "\n",
    "    return top_users_list, sorted_news_list\n",
    "\n",
    "def get_hashtag_users_df(df):\n",
    "        hashtag_users_df = df.loc[(df['User_Mentions'] != '[]') & (df['Hashtags'] != '[]')]\n",
    "        return hashtag_users_df\n",
    "\n",
    "def build_mentions_dict(df):\n",
    "\n",
    "    def make_csv(user_hashtag_dict):\n",
    "        with open('user_hashtag.csv', 'w') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            for key, value in user_hashtag_dict.items():\n",
    "                writer.writerow([key, value])\n",
    "\n",
    "    def make_lists(df):\n",
    "        user_list = list(df['User_Mentions'].split(\"'screen_name':\"))\n",
    "        hashtag_list = list(df['Hashtags'].split(\"'text':\"))\n",
    "\n",
    "\n",
    "\n",
    "    user_list = list(df['User_Mentions'].split(\"'screen_name':\"))\n",
    "    hashtag_list = list(df['Hashtags'].split(\"'text':\"))\n",
    "\n",
    "    for user in user_list:\n",
    "        stripped_user = user.split(',')[0].strip(\"' '{}[]\")\n",
    "        if stripped_user != '[]' and stripped_user != 'NA' and stripped_user != '':\n",
    "            for hashtag in hashtag_list:\n",
    "                stripped_hashtag = hashtag.split(',')[0].strip(\"' '{}[]\")\n",
    "                if stripped_hashtag != '[]' and stripped_hashtag != 'NA' and stripped_hashtag != '':\n",
    "                    dict_tuple = (stripped_user, stripped_hashtag)\n",
    "                    if(dict_tuple in user_hashtag_dict):\n",
    "                        user_hashtag_dict[dict_tuple] += 1\n",
    "                    else:\n",
    "                        user_hashtag_dict[dict_tuple] = 1\n",
    "    make_csv(user_hashtag_dict)\n",
    "    return user_hashtag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[{', \" 'rep', 'name': 'ななし', 'id': 6974932, 'id_str': '6974932', 'indices': [32, 36]}]\"] \n",
      "\n",
      "\n",
      "['[{', \" 'RepMullin', 'name': 'Markwayne Mullin', 'id': 1060370282, 'id_str': '1060370282', 'indices': [0, 10]}, {\", \" 'MichaelAvenatti', 'name': 'Michael Avenatti', 'id': 251918778, 'id_str': '251918778', 'indices': [11, 27]}, {\", \" 'DonaldJTrumpJr', 'name': 'Donald Trump Jr.', 'id': 39344374, 'id_str': '39344374', 'indices': [28, 43]}, {\", \" 'jn4ush', 'name': 'Jason Nichols for Congress', 'id': 899371613396893696, 'id_str': '899371613396893696', 'indices': [85, 92]}]\"] \n",
      "\n",
      "\n",
      "['[{', \" 'authorandijaxon', 'name': 'Andi Jaxon', 'id': 954422190442606592, 'id_str': '954422190442606592', 'indices': [74, 90]}, {\", \" 'AJAlexander16', 'name': 'AJ Alexander', 'id': 996125833193406465, 'id_str': '996125833193406465', 'indices': [91, 105]}]\"] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21     {('rep', 'oklaed'): 1, ('RepMullin', 'VoteBlue...\n",
       "161    {('rep', 'oklaed'): 1, ('RepMullin', 'VoteBlue...\n",
       "292    {('rep', 'oklaed'): 1, ('RepMullin', 'VoteBlue...\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "native_df = native_vote_csv_df()\n",
    "\n",
    "unique_df = unique_tweets(native_df)\n",
    "\n",
    "hashtag_user_df = get_hashtag_users_df(unique_df)\n",
    "\n",
    "user_hashtag_dict = {}\n",
    "\n",
    "def build_mentions_dict(df):\n",
    "    \n",
    "    def make_csv(user_hashtag_dict):\n",
    "        with open('user_hashtag.csv', 'w') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            for key, value in user_hashtag_dict.items():\n",
    "                writer.writerow([key, value])\n",
    "            \n",
    "    \n",
    "    user_list = list(df['User_Mentions'].split(\"'screen_name':\"))\n",
    "    hashtag_list = list(df['Hashtags'].split(\"'text':\"))\n",
    "    print(user_list, '\\n\\n')\n",
    "                        \n",
    "    for user in user_list:\n",
    "        stripped_user = user.split(',')[0].strip(\"' '{}[]\")\n",
    "        if stripped_user != '[]' and stripped_user != 'NA' and stripped_user != '':\n",
    "            for hashtag in hashtag_list:\n",
    "                stripped_hashtag = hashtag.split(',')[0].strip(\"' '{}[]\")\n",
    "                if stripped_hashtag != '[]' and stripped_hashtag != 'NA' and stripped_hashtag != '':\n",
    "                    dict_tuple = (stripped_user, stripped_hashtag)\n",
    "#                     print(dict_tuple)\n",
    "                    if(dict_tuple in user_hashtag_dict):\n",
    "                        user_hashtag_dict[dict_tuple] += 1\n",
    "                    else:\n",
    "                        user_hashtag_dict[dict_tuple] = 1\n",
    "    make_csv(user_hashtag_dict)\n",
    "    return user_hashtag_dict\n",
    "                        \n",
    "\n",
    "\n",
    "            \n",
    "hashtag_user_df.apply(build_mentions_dict, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('MJ_Mouton', 'BEFIERCE'), ('GreenNote', 'WhatsOn'), ('IlhanMN', 'WOMEN'), ('peggyflanagan', 'VoteBlueToSaveAmerica'), ('GovWalker', 'Alaska'), ('realDonaldTrump', 'Restart_Opposition'), ('RepAndyBiggsAZ', 'Trumpaganda'), ('smartlyjoan', 'v'), ('StittforGov', 'DontPunishSuccess'), ('firstalaskans', 'Anchorage')]\n",
      "False\n"
     ]
    },
    {
     "ename": "AmbiguousSolution",
     "evalue": "Disconnected graph: Ambiguous solution for bipartite sets.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAmbiguousSolution\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-bca897aa6476>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mbottom_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbipartite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbipartite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojected_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.7/site-packages/networkx/algorithms/bipartite/basic.py\u001b[0m in \u001b[0;36msets\u001b[0;34m(G, top_nodes)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_connected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Disconnected graph: Ambiguous solution for bipartite sets.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAmbiguousSolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_top\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_top\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAmbiguousSolution\u001b[0m: Disconnected graph: Ambiguous solution for bipartite sets."
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/tdt62/Desktop/GraduateResearch/scripts/user_hashtag.csv\", names=['User_Hashtag', 'Count'])\n",
    "df\n",
    "\n",
    "B = nx.Graph()\n",
    "# Add nodes with the node attribute \"bipartite\"\n",
    "bi1_list = []\n",
    "bi2_list = []\n",
    "for val in df['User_Hashtag'][0:10]:\n",
    "    x = val.split(',')\n",
    "#     print(x[0].strip())\n",
    "    bi1_list.append(x[0].strip(\" ''()\"))\n",
    "    bi2_list.append(x[1].strip(\" ''()\"))\n",
    "\n",
    "B.add_nodes_from(bi1_list, bipartite=0)\n",
    "B.add_nodes_from(bi2_list, bipartite=1)\n",
    "\n",
    "# print(list(zip(bi1_list, bi2_list)))\n",
    "# Add edges only between nodes of opposite node sets\n",
    "B.add_edges_from(list(zip(bi1_list, bi2_list)))\n",
    "print(B.edges())\n",
    "print(nx.is_connected(B))\n",
    "bottom_nodes, top_nodes = bipartite.sets(B)\n",
    "\n",
    "G = bipartite.projected_graph(B, top_nodes)\n",
    "nx.draw(G)\n",
    "# nx.draw(G, with_labels = True)\n",
    "# user_hashtag_dict = {}\n",
    "# df.apply(build_mentions_dict, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
