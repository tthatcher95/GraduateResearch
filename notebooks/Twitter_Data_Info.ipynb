{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from operator import itemgetter\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_deserts_csv_df(keywords_list):\n",
    "#     filepath = \"/Users/tdt62/Desktop/test_data/*clean*\"\n",
    "    filepath = \"/Users/tdt62/Desktop/test_data/2019*stream_1*\"\n",
    "\n",
    "    list_ = []\n",
    "\n",
    "    # Takes all of the csv file and makes one big dataframe\n",
    "    for name in glob.glob(filepath):\n",
    "        df = pd.read_csv(name,index_col=None, header=0, sep='\\t', names=[\"TweetID\", \"Timestamp\", \"Full_Text\", \"In_Reply_To_User_ID\", \"User_ID\", \"User_Name\", \"User_Screen_Name\", \"Coordinates\", \"Place\", \"Bounding_Box\", \"Quoted_Status_ID\", \"Retweeted_Status\", \"Hashtags\", \"URLs\", \"User_Mentions\", \"Media\", \"Language\"])\n",
    "        list_.append(df)\n",
    "\n",
    "    # filepath = \"/projects/canis/news_deserts/twitter/data/2018_11_06*clean*\"\n",
    "    # #filepath = \"/Users/tdt62/Desktop/test_data/2018_11_0[0-5]*clean*\"\n",
    "    #\n",
    "    # # Takes all of the csv file and makes one big dataframe\n",
    "    # for name in glob.glob(filepath):\n",
    "    #     df = pd.read_csv(name,index_col=None, header=0, delimiter='\\t')\n",
    "    #     list_.append(df)\n",
    "\n",
    "    # Makes the big df in memory\n",
    "    frame = pd.concat(list_, axis = 0, ignore_index = True)\n",
    "    frame.fillna(\"NA\", inplace=True)\n",
    "    \n",
    "    def check_keywords(x):\n",
    "        for keyword in keywords:\n",
    "            if(keyword.lower() in x.lower()):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "        # Make a seperate column which is True if it has the keyword, False if not\n",
    "    frame['has_keyword'] = list(map(lambda x: check_keywords(x), frame['Full_Text']))\n",
    "    \n",
    "    # Sort based on True/False values from above\n",
    "    keywords_df = frame.loc[frame['has_keyword'] == True]\n",
    "    return keywords_df\n",
    "\n",
    "def total_tweets(df):\n",
    "\n",
    "    # Gets the total number of tweets\n",
    "    total_tweets_num = df.shape[0]  # gives number of row count\n",
    "    return total_tweets_num\n",
    "\n",
    "def unique_tweets(df):\n",
    "\n",
    "    def run_isdigit(x):\n",
    "        if math.isnan(x):\n",
    "            return False\n",
    "        else:\n",
    "            return x.isdigit()\n",
    "\n",
    "    # Gets the unique tweets i.e. no retweeted status\n",
    "    df['rt_isdigit'] = list(map(lambda x: x.isdigit(), df['Retweeted_Status']))\n",
    "    unique_df = df.loc[df['rt_isdigit'] == False]\n",
    "    unique_tweet_num = total_tweets(unique_df)\n",
    "    return unique_tweet_num\n",
    "\n",
    "def unique_domains(df):\n",
    "    \n",
    "    # Checks if URLs is empty or not\n",
    "    def empty_list_check(x):\n",
    "        if(x != '[]'):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_domains(full_domain):\n",
    "        if(isinstance(full_domain, str)):\n",
    "            m = full_domain.split(\"//\")[-1].split(\"/\")[0].split('?')[0]\n",
    "            return m\n",
    "\n",
    "        else:\n",
    "            return full_domain\n",
    "\n",
    "\n",
    "    # Gets the tweets that have URLs in them i.e. URLs column is not '[]'\n",
    "    df['domains'] = list(map(lambda x: empty_list_check(x), df['URLs']))\n",
    "\n",
    "    # Gets the domains to be only the domains i.e. www.xyz.com\n",
    "    df['actual_domain'] = list(map(lambda x: get_domains(x), df['URLs']))\n",
    "\n",
    "    # Creates a df that only has domains in the tweet\n",
    "    unique_df = df.loc[df['domains'] == True]\n",
    "\n",
    "    # Get the number of domains\n",
    "    unique_domains = len(unique_df.actual_domain.unique())\n",
    "\n",
    "    domains_df = unique_df\n",
    "\n",
    "    return unique_domains, domains_df\n",
    "\n",
    "def unique_users(df):\n",
    "    unique_users = len(df.User_ID.unique())\n",
    "    return unique_users\n",
    "\n",
    "def original_content_user(df):\n",
    "\n",
    "    def run_isdigit(x):\n",
    "        if math.isnan(x):\n",
    "            return False\n",
    "        else:\n",
    "            return x.isdigit()\n",
    "\n",
    "    # Gets the unique tweets i.e. no retweeted status\n",
    "    df['rt_isdigit'] = list(map(lambda x: x.isdigit(), df['Retweeted_Status']))\n",
    "    unique_df = df.loc[df['rt_isdigit'] == False]\n",
    "    unique_users = len(unique_df.User_ID.unique())\n",
    "    return unique_users\n",
    "\n",
    "def top_news_domains():\n",
    "\n",
    "    #specify the url\n",
    "    news_source = \"https://www.alexa.com/topsites/category/Top/News\"\n",
    "\n",
    "    #Query the website and return the html to the variable 'page'\n",
    "    page = urllib.request.urlopen(news_source)\n",
    "\n",
    "    #Parse the html in the 'page' variable, and find all of the website domains\n",
    "    soup = BeautifulSoup(page, features=\"html.parser\")\n",
    "    links = soup.find_all(\"a\")\n",
    "    link_list = []\n",
    "    actual_news_list = []\n",
    "    links_dict = {}\n",
    "    for link in links:\n",
    "        link_list.append(link.get(\"href\"))\n",
    "    i = 0\n",
    "    for link in link_list:\n",
    "        if link:\n",
    "            if \"siteinfo/\" in link:\n",
    "                news_link = link.split(\"/siteinfo/\")[1]\n",
    "                links_dict[news_link] = 0\n",
    "                actual_news_list.append(news_link)\n",
    "\n",
    "    return actual_news_list, links_dict\n",
    "\n",
    "def get_top_news_domains(df, news_list, news_dict, top_val):\n",
    "\n",
    "    # Function to iterate over the dataframe\n",
    "    def iterate_domains_dict(x):\n",
    "        for element in x.split():\n",
    "            if element in news_list:\n",
    "                news_dict[x] += 1\n",
    "        return 0\n",
    "\n",
    "\n",
    "    top_list = []\n",
    "\n",
    "    # Iterates over the domain column\n",
    "    list(map(lambda x: iterate_domains_dict(x), df['actual_domain']))\n",
    "\n",
    "    # Gets the sorted news_list in descending order\n",
    "    sorted_news_list = (list(sorted(news_dict.items(), key=itemgetter(1), reverse=True)))\n",
    "\n",
    "    # Gets the top 20 results of the sorted list\n",
    "    sorted_news_list = sorted_news_list[0:top_val]\n",
    "\n",
    "    # Stores all the names in a list, so we can read correctky\n",
    "    for item in sorted_news_list:\n",
    "        top_list.append(item[0])\n",
    "\n",
    "    return top_list, sorted_news_list\n",
    "\n",
    "def get_top_domains(df, top_val):\n",
    "    top_list = []\n",
    "    domain_dict = {}\n",
    "\n",
    "    def iterate_domains(x):\n",
    "        domain_list = list(x.split())\n",
    "        for element in domain_list:\n",
    "            if element in domain_dict:\n",
    "                domain_dict[element] += 1\n",
    "            else:\n",
    "                domain_dict[element] = 1\n",
    "\n",
    "    # Create the hashtag dict\n",
    "    list(map(lambda x: iterate_domains(x), df['actual_domain']))\n",
    "\n",
    "    # Gets the sorted news_list in descending order\n",
    "    sorted_domain_list = (list(sorted(domain_dict.items(), key=itemgetter(1), reverse=True)))\n",
    "\n",
    "    # Gets the top 20 results of the sorted list\n",
    "    sorted_domain_list = sorted_domain_list[0:top_val]\n",
    "\n",
    "    # Stores all the names in a list, so we can read correctky\n",
    "    for item in sorted_domain_list:\n",
    "        top_list.append(item[0])\n",
    "\n",
    "    return top_list, sorted_domain_list\n",
    "\n",
    "def get_number_of_domains(df):\n",
    "    total_number_domains = len(df.actual_domain)\n",
    "    return total_number_domains\n",
    "\n",
    "def count_total_news(news_dict):\n",
    "    total = sum(list(news_dict.values()))\n",
    "    return total\n",
    "\n",
    "def make_domain_csv(top_news_dict):\n",
    "    with open('news_domains.csv', 'w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        for key, value in top_news_dict.items():\n",
    "            writer.writerow([key, value])\n",
    "\n",
    "def make_hashtag_csv(hashtag_dict):\n",
    "    with open('top_hashtags.csv', 'w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        for key, value in top_news_dict.items():\n",
    "            writer.writerow([key, value])\n",
    "\n",
    "def make_domain_csv(domain_dict):\n",
    "    with open('top_domains.csv', 'w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        for key, value in top_news_dict.items():\n",
    "            writer.writerow([key, value])\n",
    "\n",
    "def hashtags(df, top_val):\n",
    "\n",
    "    top_list = []\n",
    "    hashtag_dict = {}\n",
    "\n",
    "    def iterate_hashtags(x):\n",
    "        hashtag_list = list(x.split(', '))\n",
    "        for element in hashtag_list:\n",
    "            if element.strip('[]') in hashtag_dict and x != '[]':\n",
    "                hashtag_dict[element.strip('[]')] += 1\n",
    "            else:\n",
    "                hashtag_dict[element.strip('[]')] = 1\n",
    "\n",
    "    # Create the hashtag dict\n",
    "    list(map(lambda x: iterate_hashtags(x), df['Hashtags']))\n",
    "\n",
    "    # Gets the sorted news_list in descending order\n",
    "    sorted_news_list = (list(sorted(hashtag_dict.items(), key=itemgetter(1), reverse=True)))\n",
    "\n",
    "    # Gets the top 20 results of the sorted list\n",
    "    sorted_news_list = sorted_news_list[0:top_val]\n",
    "\n",
    "    # Stores all the names in a list, so we can read correctky\n",
    "    for item in sorted_news_list:\n",
    "        top_list.append(item[0])\n",
    "\n",
    "    return top_list, sorted_news_list\n",
    "\n",
    "def top_user_mentions(df, top_val):\n",
    "\n",
    "    top_users_list = []\n",
    "    hashtag_dict = {}\n",
    "\n",
    "    def iterate_user_mentions(x):\n",
    "\n",
    "        hashtag_list = list(x.split())\n",
    "        for element in hashtag_list:\n",
    "            if element.strip('{}:,').isdigit():\n",
    "                continue\n",
    "            elif element.strip('{}:') in hashtag_dict and element != '{}' and not element.strip('{}:').isdigit():\n",
    "                hashtag_dict[element.strip('{}:')] += 1\n",
    "            else:\n",
    "                hashtag_dict[element.strip('{}:')] = 1\n",
    "\n",
    "    # Create the hashtag dict\n",
    "    list(map(lambda x: iterate_user_mentions(x), df['User_Mentions']))\n",
    "\n",
    "    # Gets the sorted news_list in descending order\n",
    "    sorted_news_list = (list(sorted(hashtag_dict.items(), key=itemgetter(1), reverse=True)))\n",
    "\n",
    "    # Gets the top 20 results of the sorted list\n",
    "    sorted_news_list = sorted_news_list[0:top_val]\n",
    "\n",
    "    # Stores all the names in a list, so we can read correctky\n",
    "    for item in sorted_news_list:\n",
    "        top_users_list.append(item[0])\n",
    "\n",
    "    return top_users_list, sorted_news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# native_df = news_deserts_csv_df()\n",
    "# native_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"Midterm\",\"Vote\",\"Politics\",\"District\",\"Senator\",\"Congress\",\"elect\",\"Representative\",\"Sen\",\"Rep\",\n",
    "            \"Republican\",\"Democrat\",\"Dem\",\"Rep\",\"Gov\",\"Debates\",\"GOP\",\"Ballot\",\"Register\",\n",
    "            \"Incumbent\",\"Delegate\",\"Potus\",\"Scotus\",\"Supreme court\",\"local news\"]\n",
    "\n",
    "news_df = news_deserts_csv_df(keywords)\n",
    "# news_df = native_vote_csv_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tweets(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"Midterm\",\"Vote\",\"Politics\",\"District\",\"Senator\",\"Congress\",\"elect\",\"Representative\",\"Sen\",\"Rep\",\n",
    "            \"Republican\",\"Democrat\",\"Dem\",\"Rep\",\"Gov\",\"Debates\",\"GOP\",\"Ballot\",\"Register\",\n",
    "            \"Incumbent\",\"Delegate\",\"Potus\",\"Scotus\",\"Supreme court\",\"local news\"]\n",
    "\n",
    "# Create big df for manipulating the data\n",
    "news_df = news_deserts_csv_df(keywords)\n",
    "\n",
    "# Get the count of unique domains and the df associated\n",
    "unique_domains_count, domains_df = unique_domains(news_df)\n",
    "\n",
    "# Total number of tweets \n",
    "count_row = total_tweets(news_df)\n",
    "\n",
    "# Unique number of tweets\n",
    "unique_tweets_count = unique_tweets(news_df)\n",
    "\n",
    "# Unique users based on UserID\n",
    "unique_users_count = unique_users(news_df)\n",
    "\n",
    "# Original Content creater count\n",
    "original_content = original_content_user(news_df)\n",
    "\n",
    "# Generates the list and dict associated with the domains\n",
    "top_news_list, top_news_dict = top_news_domains()\n",
    "\n",
    "# Gets the top 10 news domains\n",
    "top_news_list, sorted_top_news_list = get_top_news_domains(domains_df, top_news_list, top_news_dict, 10)\n",
    "\n",
    "# Gets top 20 domains\n",
    "top_domain_list, sorted_top_news_list = get_top_domains(domains_df, 20)\n",
    "\n",
    "# Gets the total number of domains mentioned\n",
    "total_domains = get_number_of_domains(domains_df)\n",
    "\n",
    "# Count total number of news domains\n",
    "total_news_domains = count_total_news(top_news_dict)\n",
    "\n",
    "# Generate a CSV for the domain and it's occurence\n",
    "make_domain_csv(top_news_dict)\n",
    "\n",
    "# Gets the list for top 20 hashtags\n",
    "top_hashtags, hashtag_sorted_list = hashtags(news_df, 20)\n",
    "\n",
    "# Get the top 20 user mentions\n",
    "top_users_list, sorted_news_list = top_user_mentions(news_df, 20)\n",
    "\n",
    "# Prints each value\n",
    "print(\"Total Number of Tweets: {}\".format(count_row))\n",
    "print(\"Unique Tweets: {}\".format(unique_tweets_count))\n",
    "print(\"Unique Domains: {}\".format(unique_domains_count))\n",
    "print(\"Unique Users: {}\".format(unique_users_count))\n",
    "print(\"Original Content Users: {}\".format(original_content))\n",
    "print(\"Total Number of Domains: {}\".format(total_domains))\n",
    "print(\"Total Number of News Domains: {}\\n\".format(total_news_domains))\n",
    "print(\"Top 10 News Domains: {}\\n\".format(top_news_list))\n",
    "print(\"Top 20 Hashtags: {}\\n\".format(top_hashtags))\n",
    "print(\"Top 20 Domains: {}\\n\".format(top_domain_list))\n",
    "print(\"Top 20 User Mentions: {}\\n\".format(top_users_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Upland', 'California')\n",
      "('Upland', 'California')\n",
      "('Upland', 'California')\n",
      "('Upland', 'California')\n",
      "('Upland', 'California')\n",
      "('Upland', 'California')\n",
      "('Upland', 'California')\n",
      "('Upland', 'California')\n",
      "('Upland', 'California')\n",
      "('Upland', 'California')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "zip argument #1 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-4ce7ebde82c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mmaster_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcity_master_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m# master_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_city_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/tdt62/Desktop/test_data/2018_11_05_06_stream_1_clean.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/Users/tdt62/Desktop/GraduateResearch/scripts/location_data/test.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaster_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0mkeywords_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Location'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0mkeywords_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-4ce7ebde82c5>\u001b[0m in \u001b[0;36mfind_city_state\u001b[0;34m(TSV_file, TSV_output_file, df)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtwitter_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwitter_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mtwitter_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Location'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtwitter_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Place'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtwitter_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'City ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtwitter_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Coordinates'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtwitter_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: zip argument #1 must support iteration"
     ]
    }
   ],
   "source": [
    "### LOCATION DATA SCRIPTS ###\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "\n",
    "\n",
    "# https://simplemaps.com/data/us-cities\n",
    "\n",
    "# twitter.com/anyuser/status/541278904204668929\n",
    "\n",
    "hashtag_state_full =  ['\\'Alabama\\'', '\\'Alaska\\'', '\\'Arizona\\'', '\\'Arkansas\\'', \\\n",
    "'\\'California\\'', '\\'Colorado\\'', '\\'Connecticut\\'', '\\'Delaware\\'', '\\'Florida\\'', \\\n",
    "'\\'Georgia\\'', '\\'Hawaii\\'', '\\'Idaho\\'', '\\'Illinois\\'' , '\\'Indiana\\'' , '\\'Iowa' , \\\n",
    "'\\'Kansas\\'' , '\\'Kentucky\\'' , '\\'Louisiana\\'' , '\\'Maine\\'' , '\\'Maryland\\'' , \\\n",
    "'\\'Massachusetts\\'' , '\\'Michigan\\'' , '\\'Minnesota\\'' , '\\'Mississippi\\'' , \\\n",
    "'\\'Missouri\\'' , '\\'Montana\\'' '\\'Nebraska\\'' , '\\'Nevada' , '\\'NewHampshire\\'' , \\\n",
    "'\\'NewJersey\\'' , '\\'NewMexico\\'' , '\\'NewYork\\'' , '\\'NorthCarolina\\'' , \\\n",
    "'\\'NorthDakota\\'' , '\\'Ohio' , '\\'Oklahoma\\'' , '\\'Oregon\\'' , '\\'Pennsylvania\\'', \\\n",
    "'\\'RhodeIsland\\'', '\\'SouthCarolina\\'' , '\\'SouthDakota\\'' , '\\'Tennessee\\'' , \\\n",
    "'\\'Texas\\'' , '\\'Utah\\'' , '\\'Vermont\\'' , '\\'Virginia\\'' , '\\'Washington\\'' , \\\n",
    "'\\'WestVirginia\\'' , '\\'Wisconsin\\'', '\\'Wyoming\\'']\n",
    "\n",
    "hashtag_state_abbr = ['\\'AL\\'', '\\'AK\\'', '\\'AZ\\'', '\\'AR\\'', '\\'CA\\'', '\\'CO\\'', \\\n",
    "    '\\'CT\\'', '\\'DE\\'', '\\'FL\\'', '\\'GA\\'', '\\'HI\\'', '\\'ID\\'', '\\'IL\\'', '\\'IN\\'', \\\n",
    "    '\\'IA\\'', '\\'KS\\'', '\\'KY\\'', '\\'LA\\'', '\\'ME\\'', '\\'MD\\'', '\\'MA\\'', '\\'MI\\'', \\\n",
    "    '\\'MN\\'', '\\'MS\\'', '\\'MO\\'', '\\'MT\\'', '\\'NE\\'', '\\'NV\\'', '\\'NH\\'', '\\'NJ\\'', \\\n",
    "    '\\'NM\\'', '\\'NY\\'', '\\'NC\\'', '\\'ND\\'', '\\'OH\\'', '\\'OK\\'', '\\'OR\\'', '\\'PA\\'', \\\n",
    "    '\\'RI\\'', '\\'SC\\'', '\\'SD\\'', '\\'TN\\'', '\\'TX\\'', '\\'UT\\'', '\\'VT\\'', '\\'VA\\'', \\\n",
    "    '\\'WA\\'', '\\'WV\\'', '\\'WI\\'', '\\'WY\\'']\n",
    "\n",
    "\n",
    "\n",
    "city_state_abbr = []\n",
    "city_state_full = []\n",
    "city_list_indicies = []\n",
    "\n",
    "def make_domain_csv(domain_dict):\n",
    "    with open('top_domains.csv', 'w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        for key, value in top_news_dict.items():\n",
    "            writer.writerow([key, value])\n",
    "            \n",
    "def city_master_list():\n",
    "    \n",
    "    df = pd.read_csv('/Users/tdt62/Desktop/GraduateResearch/scripts/location_data/uscitiesv1.4.csv', index_col=None, header=0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_city_state(TSV_file, TSV_output_file, df):\n",
    "    \n",
    "    city_state_abbr = set(zip(df['lat'], zip(df['lng'], zip(df['state_id'], zip(df['city'], df['state_name'])))))\n",
    "    \n",
    "    twitter_df = pd.read_csv(TSV_file, index_col=None, header=0, delimiter='\\t')    \n",
    "    \n",
    "    location_df = pd.DataFrame(columns=['Tweet ID', 'Name Place', 'City ID'])\n",
    "    \n",
    "    def location_csv(twitter_df):\n",
    "        coordinates = []\n",
    "        ctyid = []\n",
    "        places = []\n",
    "        has_location = []\n",
    "        for city in city_state_abbr:\n",
    "            place = str(', '.join(city[1][1][1]))\n",
    "            if str(city[0]) in twitter_df['Full_Text'] or place in twitter_df['Full_Text']:\n",
    "                has_location.append(True)\n",
    "                places.append(place)\n",
    "                ctyid.append(city[1][1][0])\n",
    "                coordinates.append((city[0], city[1][0]))\n",
    "                \n",
    "        return has_location, places, ctyid, coordinates \n",
    "\n",
    "    import time\n",
    "    start = time.time()\n",
    "    twitter_df = twitter_df[0:10]\n",
    "    twitter_df['Location'], twitter_df['Place'], twitter_df['City ID'], twitter_df['Coordinates'] = list(zip(*twitter_df[0:10].apply(location_csv, axis=1)))\n",
    "    end = time.time()\n",
    "    print(\"Time: {}\".format(end - start))\n",
    "    twitter_df.to_csv(TSV_output_file, sep='\\t')\n",
    "    return twitter_df\n",
    "\n",
    "    \n",
    "master_list = city_master_list()\n",
    "# master_list\n",
    "frame = find_city_state(\"/Users/tdt62/Desktop/test_data/2018_11_05_06_stream_1_clean.csv\", \"/Users/tdt62/Desktop/GraduateResearch/scripts/location_data/test.csv\", master_list)\n",
    "keywords_df = frame.loc[frame['Location'] == True]\n",
    "keywords_df\n",
    "\n",
    "\n",
    "# def parse_args():\n",
    "#     '''\n",
    "#     Parses the command line arguments for the script\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "\n",
    "#     args : object\n",
    "#            Python arg parser object\n",
    "#     '''\n",
    "#     parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "\n",
    "#     parser.add_argument('TSV_file', type=str, help='Twitter stream TSV file to find city states.')\n",
    "\n",
    "#     parser.add_argument('TSV_output_file', type=str, help='Output file with found city states.')\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "#     return args\n",
    "\n",
    "# def main():\n",
    "#     args = parse_args()\n",
    "\n",
    "#     city_list_indicies = city_master_list()\n",
    "\n",
    "#     find_city_state(args.TSV_file, args.TSV_output_file, city_list_indicies)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'TweetID', 'Timestamp', 'Full_Text',\n",
      "       'In_Reply_To_User_ID', 'User_ID', 'User_Name', 'User_Screen_Name',\n",
      "       'Coordinates', 'Place', 'Bounding_Box', 'Quoted_Status_ID',\n",
      "       'Retweeted_Status', 'Hashtags', 'URLs', 'User_Mentions', 'Media',\n",
      "       'Language', 'Location', 'Name Place', 'City ID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(frame.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/Users/tdt62/Desktop/test_data/2018_12*stream_1*\"\n",
    "\n",
    "list_ = []\n",
    "\n",
    "# Takes all of the csv file and makes one big dataframe\n",
    "for name in glob.glob(filepath):\n",
    "    with open(name, newline='') as csvfile:\n",
    "        dialect = csv.Sniffer().sniff(csvfile.read(10))\n",
    "        csvfile.seek(0)\n",
    "        reader = csv.reader(csvfile, dialect)\n",
    "        print(dialect.delimiter)\n",
    "\n",
    "#     df = pd.read_csv(name,index_col=None, header=0, delimiter='\\t')\n",
    "#     list_.append(df)\n",
    "\n",
    "\n",
    "    # ... process CSV file contents here ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
